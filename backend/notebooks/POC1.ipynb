{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.utils.llm.helpers import get_openai_api_key\n",
    "\n",
    "# Get the OpenAI API key\n",
    "openai_api_key = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from app.utils.llm.helpers import init_llm_configurations\n",
    "from app.settings import Constants\n",
    "\n",
    "init_llm_configurations(llm_model=Constants.LLM_MODEL, embedding_model=Constants.EMBEDDING_MODEL)\n",
    "\n",
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow asyncio in Jupyter Notebook\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file pdf\n",
    "\n",
    "### Behavior\n",
    "\n",
    "Assume that already has the pdf file.\n",
    "\n",
    "- Use markitdown to convert pdf to markdown\n",
    "- Create a Document object of LlamaIndex from the markdown file\n",
    "- Create an Ingestion Pipeline (cache enabled) and ingest the Document to Node objects\n",
    "- Save those nodes to the storage context including docstore, vectorstore, and index store\n",
    "\n",
    "### Chunking method\n",
    "\n",
    "- Firstly, having the document with Vietnamese text\n",
    "- Translate the document to English\n",
    "- Use [semantic splitter](https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_chunking/) to split the document into chunks\n",
    "  - Threshold: 85\n",
    "  - Buffer size: 3\n",
    "  - Why?\n",
    "\n",
    "References:\n",
    "- https://youtu.be/8OJC21T2SL4?t=1933\n",
    "\n",
    "Other methods:\n",
    "- [Semantic Double Merging Chunking](https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_double_merging_chunking/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.integrations.llama_index.ingestion_pipelines.readers import MarkitdownReader\n",
    "\n",
    "# Initialize the MarkitdownReader\n",
    "markitdown_reader = MarkitdownReader()\n",
    "filepath = \"data/NQLD01.pdf\"\n",
    "\n",
    "documents = markitdown_reader.load_data(filepath)\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation from Vietnamese to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.integrations.llama_index.ingestion_pipelines.translators import Translator\n",
    "\n",
    "translator = Translator.from_defaults(source_language=\"vietnamese\", target_language=\"english\")\n",
    "translated_documents = translator.get_translated_documents(documents, show_progress=True)\n",
    "\n",
    "translated_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "# Text splitters\n",
    "# Use the SemanticSplitterNodeParser to split the text into nodes\n",
    "semantic_splitter = SemanticSplitterNodeParser.from_defaults(\n",
    "    embed_model=Settings.embed_model,\n",
    "    breakpoint_percentile_threshold=85,\n",
    "    buffer_size=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metadata key: issue_date, is_outdated\n",
    "Input from user input on frontend.\n",
    "Purpose: To filter out outdated documents.\n",
    "\"\"\"\n",
    "issue_data = input(\"Enter the issue date of the document (YYYY-MM-DD): \") or \"2020-01-01\"\n",
    "is_outdated = bool(input(\"Is the document outdated? (True/False): \")) or False\n",
    "\n",
    "for document in translated_documents:\n",
    "    document.metadata[\"issue_date\"] = issue_data\n",
    "    document.metadata[\"is_outdated\"] = is_outdated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metadata key: excerpt_keywords\n",
    "Extract keywords from the text\n",
    "Purpose: Can be used to do topic/tag or keyword-based search (metadata filter).\n",
    "\"\"\"\n",
    "from llama_index.core.extractors import KeywordExtractor\n",
    "\n",
    "keyword_extractor = KeywordExtractor(llm=Settings.llm, keywords=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "from llama_index.core.extractors import SummaryExtractor\n",
    "\n",
    "summary_extractor = SummaryExtractor(summaries=[\"prev\", \"self\", \"next\"], llm=Settings.llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Transformations\n",
    "# TODO: add a cleaner to cleanup llm's generated text, e.g. here's a summary of the document: ...\n",
    "transformations = []\n",
    "transformations.append(semantic_splitter)\n",
    "transformations.append(keyword_extractor)\n",
    "transformations.append(summary_extractor)\n",
    "transformations.append(Settings.embed_model)\n",
    "\n",
    "# Initialize the ingestion pipeline\n",
    "pipeline = IngestionPipeline(transformations=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "nodes = await pipeline.arun(documents=translated_documents, show_progress=True)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total nodes: {len(nodes)}\")\n",
    "for node in nodes:\n",
    "    print(f\"================== {node.id_} ========================\")\n",
    "    print(node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total nodes: {len(nodes)}\")\n",
    "for node in nodes:\n",
    "    print(f\"================== {node.id_} ========================\")\n",
    "    print(node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total nodes: {len(nodes)}\")\n",
    "for node in nodes:\n",
    "    print(f\"================== {node.id_} ========================\")\n",
    "    print(f\"Dimensions: {len(node.embedding)}\")\n",
    "    print(node.embedding[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_nodes = []\n",
    "# Duplicate to number of nodes to 10x\n",
    "for node in nodes:\n",
    "    for _ in range(1000):\n",
    "        duplicate_nodes.append(node)\n",
    "len(duplicate_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from qdrant_client.http.models import VectorParams\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "vector_params = VectorParams(size=768, distance=\"Cosine\")\n",
    "\n",
    "client = qdrant_client.QdrantClient(location=\":memory:\") # In-memory qdrant\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test_poc1\", dense_config=vector_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Indexing\n",
    "\n",
    "## Behavior\n",
    "\n",
    "There're 2 ways to create an index:\n",
    "- Load from transformed nodes (first time)\n",
    "- Load from vector store - qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build index from nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "index = VectorStoreIndex(nodes=duplicate_nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load index from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between index or loaded_index\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "question = \"company\"\n",
    "\n",
    "response = query_engine.query(question)\n",
    "display_response(response, show_source=True, show_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "import Stemmer\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=1,\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "retrieved_nodes = bm25_retriever.retrieve(\n",
    "    \"What do you know?\"\n",
    ")\n",
    "for node in retrieved_nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceEmbeddingOptimizer\n",
    "from llama_index.core.postprocessor import EmbeddingRecencyPostprocessor\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "\n",
    "node_postprocessors = [\n",
    "    SentenceEmbeddingOptimizer(\n",
    "        embed_model=Settings.embed_model,\n",
    "        # percentile_cutoff=0.5,\n",
    "        threshold_cutoff=0.7,\n",
    "    ),\n",
    "    EmbeddingRecencyPostprocessor(date_key=\"date\", similarity_cutoff=0.7),\n",
    "    LLMRerank(top_n=2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "\n",
    "# Configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(llm=Settings.llm, response_mode=ResponseMode.COMPACT)\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=bm25_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=node_postprocessors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "CUSTOM_PROMPT = PromptTemplate(\n",
    "    \"\"\"\n",
    "    Based on the conversation history between the User and the Assistant, along with the User's new question, analyze and understand the question within the context of the conversation.\n",
    "    Provide a relevant response in Vietnamese, using a professional tone like a Human Resource Specialist.  \n",
    "\n",
    "    <Conversation History>\n",
    "    {chat_history}\n",
    "\n",
    "    <Current Question>\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "custom_chat_history = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Hello assistant, we are having a conversation about the company's regulations.\",\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=\"Great, would you like to know more information about the company's regulations?\",\n",
    "    ),\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
