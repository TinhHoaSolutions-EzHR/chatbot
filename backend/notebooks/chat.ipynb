{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.utils.llm.helpers import init_llm_configurations\n",
    "from app.settings import Constants\n",
    "\n",
    "\n",
    "init_llm_configurations(llm_model=Constants.LLM_MODEL, embedding_model=Constants.EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"../examples\").load_data(show_progress=True)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index\n",
    "index = VectorStoreIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a human resources professional at a company that needs to quickly find information in its policy documents.\n",
    "\n",
    "Guidelines:\n",
    "- Provide information explicitly stated in policy documents\n",
    "- For basic contextual questions (company name, policy type, etc), use ONLY information directly visible in the provided context\n",
    "- No assumptions or interpretations about policy details\n",
    "- Do not explain\n",
    "- Your primary language is Vietnamese\n",
    "- If you cannot find an answer, please output \"Không tìm thấy thông tin, hãy liên hệ HR. SĐT: 0123456789 hoặc email contact.hr@company.com.\"\n",
    "- If the response is empty, please answer with the knowledge you have\n",
    "\n",
    "Let's work this out in a step by step way to be sure we have the right answer.\n",
    "Answer as the tone of a human resources professional, be polite and helpful.\"\"\"\n",
    "\n",
    "CONTEXT_PROMPT = \"\"\"\n",
    "The following is a friendly conversation between an employee and a human resources professional.\n",
    "The professional is talkative and provides lots of specific details from her context.\n",
    "If the professional does not know the answer to a question, she truthfully says she does not know.\n",
    "\n",
    "Here are the relevant documents for the context:\n",
    "'''\n",
    "{context_str}\n",
    "'''\n",
    "\n",
    "## Instruction\n",
    "Based on the above documents, provide a detailed answer for the employee question below.\n",
    "Answer \"don't know\" if not present in the document.\"\"\"\n",
    "\n",
    "CONTEXT_REFINE_PROMPT = \"\"\"\n",
    "The following is a friendly conversation between an employee and a human resources professional.\n",
    "The professional is talkative and provides lots of specific details from her context.\n",
    "If the professional does not know the answer to a question, she truthfully says she does not know.\n",
    "\n",
    "Here are the relevant documents for the context:\n",
    "'''\n",
    "{context_msg}\n",
    "'''\n",
    "\n",
    "Existing Answer:\n",
    "'''\n",
    "{existing_answer}\n",
    "'''\n",
    "\n",
    "## Instruction\n",
    "Refine the existing answer using the provided context to assist the user.\n",
    "If the context isn't helpful, just repeat the existing answer and nothing more.\"\"\"\n",
    "\n",
    "CONDENSE_PROMPT = \"\"\"\n",
    "Given the following conversation between an employee and a human resources professional and a follow up question from the employee.\n",
    "Your task is to firstly summarize the chat history and secondly condense the follow up question into a standalone question.\n",
    "\n",
    "Chat History:\n",
    "'''\n",
    "{chat_history}\n",
    "'''\n",
    "\n",
    "Follow Up Input:\n",
    "'''\n",
    "{question}\n",
    "'''\n",
    "\n",
    "Output format: a standalone question.\n",
    "\n",
    "Your response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever and the chat engine\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.memory import BaseMemory\n",
    "\n",
    "\n",
    "retriever = index.as_retriever()\n",
    "memory = BaseMemory.from_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    context_prompt=CONTEXT_PROMPT,\n",
    "    context_refine_prompt=CONTEXT_REFINE_PROMPT,\n",
    "    condense_prompt=CONDENSE_PROMPT,\n",
    "    node_postprocessors=[LLMRerank(top_n=5)],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"trường hợp nghỉ việc riêng không lương?\",\n",
    "    \"mỗi năm được nghỉ mấy ngày?\",\n",
    "    \"nghỉ phép năm nghỉ bao nhiêu ngày?\",\n",
    "    \"trường hợp xảy ra mất tài sản thì tôi phải đền bù làm sao?\",\n",
    "    \"bạn là ai?\",\n",
    "    \"giờ là lúc nhìn lại xem một năm ù í a\",\n",
    "    \"tôi muốn nghỉ việc, tôi phải làm thế nào?\",\n",
    "    \"công ty có trách nhiệm gì đối với tôi?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    response = chat_engine.chat(question)\n",
    "    print(f\"Answer: {response}\")\n",
    "    print(\"============================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
