{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (done) import library & define env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-vector-stores-qdrant\n",
    "# %pip install --upgrade llama-index\n",
    "# %pip install --upgrade fastembed\n",
    "# %pip install --upgrade llama-index-storage-chat-store-redis\n",
    "# %pip install --upgrade llama-index-core\n",
    "# %pip install --upgrade redis\n",
    "# %pip install --upgrade llama-index-storage-kvstore-redis\n",
    "# %pip install \"numpy<2.0\"\n",
    "# %pip install --upgrade pandas pyarrow llama-index\n",
    "# %pip install \"pybind11>=2.12\"\n",
    "# %pip install --upgrade llama-index-retrievers-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (done) read and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processed: TULD02.pdf\n",
      "Invalid date format. Defaulting to today's date.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pdfplumber\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "def process_single_pdf(pdf_path: str) -> List:\n",
    "    \"\"\"\n",
    "    Load and parse a specific PDF file using SimpleDirectoryReader.\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file to process.\n",
    "\n",
    "    Returns:\n",
    "        List: Documents from SimpleDirectoryReader.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "\n",
    "    if not pdf_path.lower().endswith('.pdf'):\n",
    "        raise ValueError(f\"File {pdf_path} is not a PDF file\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        try:\n",
    "            pdf_filename = os.path.basename(pdf_path)\n",
    "            temp_text_file = os.path.join(temp_dir, f'{os.path.splitext(pdf_filename)[0]}.txt')\n",
    "\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                text_parts = []\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text_parts.append(page_text)\n",
    "\n",
    "            with open(temp_text_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(''.join(text_parts))\n",
    "\n",
    "            print(f\"File processed: {pdf_filename}\")\n",
    "\n",
    "            documents = SimpleDirectoryReader(temp_dir).load_data()\n",
    "\n",
    "            issue_date_str = input(\"Please enter the issue date (YYYY-MM-DD): \")\n",
    "            try:\n",
    "                issue_date = datetime.strptime(issue_date_str, \"%Y-%m-%d\")\n",
    "                issue_date_str = issue_date.strftime(\"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid date format. Defaulting to today's date.\")\n",
    "                issue_date_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            outdated_input = input(\"Is this document outdated? (yes/no): \").strip().lower()\n",
    "            outdated = True if outdated_input == \"yes\" else False\n",
    "\n",
    "            for doc in documents:\n",
    "                doc.metadata['issue_date'] = issue_date_str\n",
    "                doc.metadata['outdated'] = outdated\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing file {pdf_filename}: {str(e)}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "input_dir = \"TULD02.pdf\"\n",
    "documents = process_single_pdf(input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (done) define qdrant & docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current collections: collections=[]\n",
      "Collection 'documents_collection' đã được tạo.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams\n",
    "\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "collection_name = \"documents_collection\"\n",
    "\n",
    "collections = qdrant_client.get_collections()\n",
    "print(\"Current collections:\", collections)\n",
    "\n",
    "if collection_name not in [collection.name for collection in collections.collections]:\n",
    "    vector_params = VectorParams(size=1536, distance=\"Cosine\")\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=vector_params\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' đã được tạo.\")\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' đã tồn tại.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client, \n",
    "    collection_name=\"documents_collection\"\n",
    ")\n",
    "docstore = SimpleDocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (done) process\n",
    "- indexing\n",
    "- chunking\n",
    "- pipeline (save to qdrant & save to docstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8b7f09fe9a4cd2bbc2083bd6361724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1274bf8170ba4828b52ff284153710b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.56it/s]\n",
      "100%|██████████| 7/7 [00:01<00:00,  4.93it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7c181da7ad43b090423c2460269515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import QuestionsAnsweredExtractor, KeywordExtractor\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.core import Settings\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\"\"\"Processes a set of documents through an ingestion pipeline, extracts semantic and keyword information,\n",
    "and stores the processed nodes in a vector database.\n",
    "\n",
    "Steps:\n",
    "1. Splits the input documents into semantic chunks using `SemanticSplitterNodeParser`.\n",
    "2. Extracts questions and keywords from the chunks using `QuestionsAnsweredExtractor` and `KeywordExtractor`.\n",
    "3. Stores the processed nodes in a vector database (e.g., Qdrant).\n",
    "\n",
    "Args:\n",
    "    documents (List[Document]): The input documents to be processed. Each document is typically \n",
    "        an instance of a class containing text and optional metadata.\n",
    "\n",
    "    vector_store (VectorStore): The vector store where processed nodes will be stored. This could \n",
    "        be a Qdrant vector store or similar.\n",
    "\n",
    "Returns:\n",
    "    List[Node]: A list of processed nodes containing semantic chunks, extracted keywords, \n",
    "    and associated metadata.\n",
    "\n",
    "Raises:\n",
    "    Exception: If the pipeline encounters issues during document processing or saving to the vector store.\"\"\"\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "extractors = [\n",
    "    QuestionsAnsweredExtractor(llm=Settings.llm, questions=1),\n",
    "    KeywordExtractor(llm=Settings.llm, keywords=5),\n",
    "]\n",
    "\n",
    "transformations = [splitter] + extractors + [embed_model]\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=transformations,\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "# run pipeline\n",
    "nodes = pipeline.run(documents=documents, show_progress=True, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (done) only bm25 (answer right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    }
   ],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "import Stemmer\n",
    "\n",
    "# We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=2,\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 33f9de68-f8ff-400e-8160-ff4b24282911<br>**Similarity:** 1.4459148645401<br>**Text:** 2. Đại diện tập thể người lao động:\r\n",
       "Ông: NGUYỄN DANH TÙNG - Chủ tịch Công đoàn Tổng Công ty.\r\n",
       "Hai bên cam kết thực hiện đúng các quy định của pháp luật và cùng nhau\r\n",
       "thỏa thuận ký kết Thỏa ước lao động tập thể, gồm 03 chương và 16 điều;\r\n",
       "CHƯƠNG I\r\n",
       "NHỮNG QUY ĐỊNH CHUNG\r\n",
       "Thoả ước lao động tập thể này quy định mối quan hệ lao động giữa tập thể\r\n",
       "lao động và NSDLĐ về các điều kiện lao động và sử dụng lao động, quyền và\r\n",
       "nghĩa vụ của mỗi bên trong thời hạn Thỏa ước có hiệu lực. Mọi trường hợp khác\r\n",
       "trong mối quan hệ lao động không quy định trong bản Thỏa ước lao động tập thể\r\n",
       "này, sẽ được giải quyết theo Bộ luật Lao động và các văn bản quy phạm pháp\r\n",
       "luật hiện hành.\r\n",
       "Điều 1. Đối tượng thi hành:\r\n",
       "Thỏa ước lao động tập thể này áp dụng đối với:\r\n",
       "- Người sử dụng lao động (NSDLĐ); Bao gồm Hội đồng Quản trị, Ban\r\n",
       "Tổng giám đốc.\r\n",
       "- Tập thể người lao động (NLĐ): Bao gồm người lao động đang làm việc\r\n",
       "tại Tổng công ty, kể cả người lao động trong thời gian học nghề, thử việc, NLĐ\r\n",
       "vào làm việc sau ngày Thỏa ước có hiệu lực.\r\n",
       "- Ban Chấp hành công đoàn Tổng công ty.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e96e9eb5-ec17-4176-860b-602794cc8cf7<br>**Similarity:** 1.2385129928588867<br>**Text:** Một số thỏa thuận khác.\r\n",
       "Tùy theo hiệu quả sản xuất kinh doanh hàng năm và phần lợi nhuận được\r\n",
       "trích để lại các quỹ cho Tổng công ty. Hội đồng Quản trị, Ban Tổng giám đốc\r\n",
       "Tổng công ty sẽ thống nhất với BCH công đoàn cơ sở các nội dung như sau:\r\n",
       "- Chi thăm hỏi cho cán bộ hưu trí; NLĐ, thân nhân là thương binh, liệt sỹ,\r\n",
       "gia đình có công cách mạng vào dịp 27 tháng 7 và tết nguyên đán…\r\n",
       "- Tổ chức tặng quà động viên con NLĐ học đạt từ loại giỏi trở lên nhân\r\n",
       "dịp khai trường; thiếu niên, nhi đồng nhân các ngày quốc tế thiếu nhi, tết trung\r\n",
       "thu.\r\n",
       "- Thăm hỏi gia đình NLĐ; (bị bệnh hiểm nghèo hoặc khi có hữu sự).\r\n",
       "- Trợ cấp đột xuất NLĐ khi gặp sự cố rủi ro: Thiên tai, hỏa hoạn, bệnh\r\n",
       "hiểm nghèo, tai nạn lao động, tai nạn giao thông mất sức lao động từ 31% trở\r\n",
       "lên.\r\n",
       "- Chi phí cho các hoạt động văn hóa, thể thao theo chương trình kế hoạch\r\n",
       "hàng năm do các đoàn thể đề xuất được thống nhất của Ban Tổng giám đốc\r\n",
       "Tổng công ty.\r\n",
       "- Có chính sách đối với nguồn nhân lực có trình độ và chính sách thu hút\r\n",
       "nguồn lao động đáp ứng sự nghiệp phát triển công ty trong từng giai đoạn.\r\n",
       "- Có chính sách đối với người có đóng góp, hỗ trợ hoạt động Tổng công ty\r\n",
       "đem lại hiệu quả cao.\r\n",
       "- Tùy theo nguồn quỹ phúc lợi tập thể hiện còn hàng năm, BCH Công\r\n",
       "đoàn Tổng công ty đề xuất với Hội đồng Quản trị, Ban Tổng giám đốc xem xét\r\n",
       "tổ chức cho các đối tượng được khen thưởng, có sáng kiến kinh nghiệm được\r\n",
       "xét và công nhận loại A, có nhiều đóng góp tích cực cho Tổ chức Đảng bộ, Tổng\r\n",
       "công ty, Tổ chức Công đoàn và Đoàn cơ sở Tổng công ty đi thăm quan, du lịch;\r\n",
       "địa điểm thời gian sẽ do lãnh đạo Quyết định\r\n",
       "Điều 13. Tranh chấp lao động\r\n",
       "Hai bên thực hiện theo quy định tại chương XIV Bộ luật Lao động.\r\n",
       "CHƯƠNG III: ĐIỀU KHOẢN THI HÀNH\r\n",
       "Điều 14. Trách nhiệm thi hành Thỏa ước:\r\n",
       "- NSDLĐ, Ban chấp hành CĐCS có trách nhiệm triển khai, quán triệt đến\r\n",
       "tất cả NLĐ trong Tổng Công ty biết và thực hiện đúng các nội dung đã thỏa\r\n",
       "thuận trong bản Thỏa ước này.\r\n",
       "- Trong quá trình thực hiện Thỏa ước lao động tập thể, NSDLĐ và BCH\r\n",
       "CĐCS cùng nhau xem xét kiểm tra việc thực hiện Thỏa ước, nếu có nội dung\r\n",
       "chưa phù hợp thì một trong hai bên đề nghị điều chỉnh theo qui định.\r\n",
       "5Điều 15. Giải quyết tranh chấp liên quan đến các thỏa thuận trong Thỏa\r\n",
       "ước.\r\n",
       "- Các tranh chấp liên quan đến nội dung thỏa ước là tranh chấp lao động và\r\n",
       "được giải quyết theo quy định của pháp luật lao động. Mọi tranh chấp (cá nhân,\r\n",
       "tập thể), NLĐ đều phải gửi yêu cầu bằng văn bản đến Ban chấp hành CĐCS\r\n",
       "hoặc người có thẩm quyền (Ban Tổng giám đốc) để được xem xét giải quyết.\r\n",
       "- Khi có tranh chấp phát sinh trong quan hệ lao động, các bên tranh chấp thì\r\n",
       "BCH Công đoàn và người có thẩm quyền giải quyết tranh chấp sử dụng thỏa\r\n",
       "ước này và các quy định được thừa nhận trong bản thỏa ước này làm cơ sở pháp\r\n",
       "lý để xem xét vụ việc.\r\n",
       "Điều 16.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "retrieved_nodes = bm25_retriever.retrieve(\n",
    "    \"Đối tượng thi hành\"\n",
    ")\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (done) QueryFusionRetriever (answer right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=docstore,\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex(nodes=nodes,vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    # call 2 type retrieve\n",
    "    [\n",
    "        index.as_retriever(similarity_top_k=2),\n",
    "        BM25Retriever.from_defaults(\n",
    "            docstore=index.docstore, similarity_top_k=2\n",
    "        ),\n",
    "    ],\n",
    "    # max query\n",
    "    num_queries=2,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đối tượng thi hành Thỏa ước lao động tập thể này bao gồm:\n",
      "\n",
      "- Người sử dụng lao động (NSDLĐ), bao gồm Hội đồng Quản trị và Ban Tổng giám đốc.\n",
      "- Tập thể người lao động (NLĐ), bao gồm người lao động đang làm việc tại Tổng công ty, kể cả người lao động trong thời gian học nghề, thử việc, và NLĐ vào làm việc sau ngày Thỏa ước có hiệu lực.\n",
      "- Ban Chấp hành công đoàn Tổng công ty.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever)\n",
    "\n",
    "response = query_engine.query(\"Đối tượng thi hành\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (still fixing) chat engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceEmbeddingOptimizer\n",
    "from llama_index.core.postprocessor import EmbeddingRecencyPostprocessor\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "node_postprocessors = [\n",
    "    SentenceEmbeddingOptimizer(\n",
    "        embed_model=Settings.embed_model,\n",
    "        threshold_cutoff=0.7,\n",
    "    ),\n",
    "    EmbeddingRecencyPostprocessor(date_key=\"date\", similarity_cutoff=0.7),\n",
    "    LLMRerank(top_n=2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(llm=Settings.llm, response_mode=ResponseMode.COMPACT)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=node_postprocessors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Optimizer returned zero sentences.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĐối tượng thi hành\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query(str_or_query_bundle)\n\u001b[0;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:178\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    176\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    177\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 178\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m    179\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    180\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[0;32m    181\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[0;32m    183\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:134\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    133\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retriever\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:127\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._apply_node_postprocessors\u001b[1;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_node_postprocessors\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, nodes: List[NodeWithScore], query_bundle: QueryBundle\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_postprocessor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_postprocessors:\n\u001b[1;32m--> 127\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m node_postprocessor\u001b[38;5;241m.\u001b[39mpostprocess_nodes(\n\u001b[0;32m    128\u001b[0m             nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle\n\u001b[0;32m    129\u001b[0m         )\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\postprocessor\\types.py:54\u001b[0m, in \u001b[0;36mBaseNodePostprocessor.postprocess_nodes\u001b[1;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_nodes(nodes, query_bundle)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\postprocessor\\optimizer.py:138\u001b[0m, in \u001b[0;36mSentenceEmbeddingOptimizer._postprocess_nodes\u001b[1;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[0;32m    128\u001b[0m top_similarities, top_idxs \u001b[38;5;241m=\u001b[39m get_top_k_embeddings(\n\u001b[0;32m    129\u001b[0m     query_embedding\u001b[38;5;241m=\u001b[39mquery_bundle\u001b[38;5;241m.\u001b[39membedding,\n\u001b[0;32m    130\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39mtext_embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m     similarity_cutoff\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(top_idxs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer returned zero sentences.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    140\u001b[0m rangeMin, rangeMax \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(split_text)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_before \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Optimizer returned zero sentences."
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Đối tượng thi hành\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "CUSTOM_PROMPT = PromptTemplate(\n",
    "    \"\"\"\n",
    "    Based on the conversation history between the User and the Assistant, along with the User's new question, analyze and understand the question within the context of the conversation.\n",
    "    Provide a relevant response in Vietnamese, using a professional tone like a Human Resource Specialist.  \n",
    "\n",
    "    <Conversation History>\n",
    "    {chat_history}\n",
    "\n",
    "    <Current Question>\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "custom_chat_history = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Hello assistant, we are having a conversation about the company's regulations.\",\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=\"Great, would you like to know more information about the company's regulations?\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=CUSTOM_PROMPT,\n",
    "    chat_history=custom_chat_history,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Optimizer returned zero sentences.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcho tôi biết Đối tượng thi hành là ai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m chat_engine\u001b[38;5;241m.\u001b[39mchat(query)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\chat_engine\\condense_question.py:197\u001b[0m, in \u001b[0;36mCondenseQuestionChatEngine.chat\u001b[1;34m(self, message, chat_history)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_engine\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39m_streaming \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Query with standalone question\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m query_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_engine\u001b[38;5;241m.\u001b[39mquery(condensed_question)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# NOTE: reset streaming flag\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_engine, RetrieverQueryEngine):\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query(str_or_query_bundle)\n\u001b[0;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:178\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    176\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[0;32m    177\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 178\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m    179\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[0;32m    180\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[0;32m    181\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[0;32m    183\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:134\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    133\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retriever\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:127\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._apply_node_postprocessors\u001b[1;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_node_postprocessors\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, nodes: List[NodeWithScore], query_bundle: QueryBundle\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node_postprocessor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_postprocessors:\n\u001b[1;32m--> 127\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m node_postprocessor\u001b[38;5;241m.\u001b[39mpostprocess_nodes(\n\u001b[0;32m    128\u001b[0m             nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle\n\u001b[0;32m    129\u001b[0m         )\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\postprocessor\\types.py:54\u001b[0m, in \u001b[0;36mBaseNodePostprocessor.postprocess_nodes\u001b[1;34m(self, nodes, query_bundle, query_str)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_nodes(nodes, query_bundle)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\LUNE\\anaconda3\\Lib\\site-packages\\llama_index\\core\\postprocessor\\optimizer.py:138\u001b[0m, in \u001b[0;36mSentenceEmbeddingOptimizer._postprocess_nodes\u001b[1;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[0;32m    128\u001b[0m top_similarities, top_idxs \u001b[38;5;241m=\u001b[39m get_top_k_embeddings(\n\u001b[0;32m    129\u001b[0m     query_embedding\u001b[38;5;241m=\u001b[39mquery_bundle\u001b[38;5;241m.\u001b[39membedding,\n\u001b[0;32m    130\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39mtext_embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m     similarity_cutoff\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(top_idxs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer returned zero sentences.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    140\u001b[0m rangeMin, rangeMax \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(split_text)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_before \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Optimizer returned zero sentences."
     ]
    }
   ],
   "source": [
    "query = \"cho tôi biết Đối tượng thi hành là ai\"\n",
    "response = chat_engine.chat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mọi cá nhân làm việc trong công ty, bao gồm nhân viên, quản lý và các bên liên quan khác, đều có trách nhiệm tuân thủ các quy định đã được ban hành. Điều này đảm bảo rằng tất cả các cấp bậc trong tổ chức đều thực hiện đúng các nội dung trong thỏa ước lao động và các quy định liên quan. Nếu cần thêm thông tin chi tiết về từng đối tượng cụ thể hoặc các quy định liên quan, hãy cho biết để có thể cung cấp thêm.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
