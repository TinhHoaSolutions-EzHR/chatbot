{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-vector-stores-qdrant\n",
    "# %pip install --upgrade llama-index\n",
    "# %pip install --upgrade fastembed\n",
    "# %pip install --upgrade llama-index-storage-chat-store-redis\n",
    "# %pip install --upgrade llama-index-core\n",
    "# %pip install --upgrade redis\n",
    "# %pip install --upgrade llama-index-storage-kvstore-redis\n",
    "# %pip install \"numpy<2.0\"\n",
    "# %pip install --upgrade pandas pyarrow llama-index\n",
    "# %pip install \"pybind11>=2.12\"\n",
    "# %pip install --upgrade llama-index-retrievers-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Tải các biến môi trường từ file .env\n",
    "load_dotenv()\n",
    "\n",
    "# Lấy giá trị khóa API từ biến môi trường\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# Initialize the LLM embeddings\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "# Global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.context_window = 128_000\n",
    "Settings.num_output = 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pdfplumber\n",
    "from typing import List\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "def process_single_pdf(pdf_path: str) -> List:\n",
    "    \"\"\"\n",
    "    Load and parse a specific PDF file using SimpleDirectoryReader.\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file to process.\n",
    "\n",
    "    Returns:\n",
    "        List: Documents from SimpleDirectoryReader.\n",
    "    \"\"\"\n",
    "    # Check file existence\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "\n",
    "    # Check file extension\n",
    "    if not pdf_path.lower().endswith('.pdf'):\n",
    "        raise ValueError(f\"File {pdf_path} is not a PDF file\")\n",
    "\n",
    "    # Create temporary directory\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        try:\n",
    "            pdf_filename = os.path.basename(pdf_path)\n",
    "            temp_text_file = os.path.join(temp_dir, f'{os.path.splitext(pdf_filename)[0]}.txt')\n",
    "\n",
    "            # Load PDF and save to temporary text file\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                text_parts = []  # Use a list to gather text parts\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:  # Only add if there's text\n",
    "                        text_parts.append(page_text)\n",
    "\n",
    "            # Write all text at once\n",
    "            with open(temp_text_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(''.join(text_parts))\n",
    "\n",
    "            print(f\"File processed: {pdf_filename}\")\n",
    "\n",
    "            # Use SimpleDirectoryReader to read text files\n",
    "            documents = SimpleDirectoryReader(temp_dir).load_data()\n",
    "\n",
    "        except pdfplumber.PDFException as e:\n",
    "            raise Exception(f\"PDF processing error for file {pdf_filename}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error processing file {pdf_filename}: {str(e)}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "input_dir = \"TULD02.pdf\"\n",
    "documents = process_single_pdf(input_dir)\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams\n",
    "\n",
    "# Kết nối tới Qdrant (đang chạy cục bộ hoặc cloud)\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "collection_name = \"documents_collection\"\n",
    "\n",
    "# Kiểm tra kết nối và danh sách các collections hiện có\n",
    "collections = qdrant_client.get_collections()\n",
    "print(\"Current collections:\", collections)\n",
    "\n",
    "# Kiểm tra xem collection đã tồn tại hay chưa\n",
    "if collection_name not in [collection.name for collection in collections.collections]:\n",
    "    # Tạo collection nếu chưa tồn tại\n",
    "    vector_params = VectorParams(size=1536, distance=\"Cosine\")  # Kích thước vector và khoảng cách cosine\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=vector_params,\n",
    "        on_disk = True\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' đã được tạo.\")\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' đã tồn tại.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_to_qdrant(nodes):\n",
    "    \n",
    "    for node in nodes:\n",
    "        text = node.text\n",
    "\n",
    "        # Get metadata to save as payload dict\n",
    "        metadata = node.metadata\n",
    "        payload = dict(metadata)\n",
    "        \n",
    "        # Create embdding from text\n",
    "        embedding = embed_model._get_text_embedding(text)\n",
    "    \n",
    "\n",
    "        try:\n",
    "            qdrant_client.upsert(\n",
    "                collection_name=\"documents_collection\",\n",
    "                points=[\n",
    "                    PointStruct(\n",
    "                        id=node.id_,\n",
    "                        vector=embedding,\n",
    "                        payload=payload\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            print(\"Dữ liệu đã được lưu vào Qdrant!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to Qdrant: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Kiểm tra xem collection có tồn tại không\n",
    "try:\n",
    "    # Lấy thông tin về collection\n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"Collection '{collection_name}' đã được kết nối thành công.\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi kết nối với collection: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# Khởi tạo QdrantVectorStore với tên collection được cập nhật\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client, \n",
    "    collection_name=\"documents_collection\"  # Đổi tên collection tại đây\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "print(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# indexing & chunking & pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.extractors import QuestionsAnsweredExtractor, KeywordExtractor\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.core import Settings\n",
    "from qdrant_client import QdrantClient\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Tạo vector_store sử dụng Qdrant\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=\"documents_collection\")\n",
    "\n",
    "# Cấu hình các extractor và node parser\n",
    "extractors = [\n",
    "    QuestionsAnsweredExtractor(llm=Settings.llm, questions=1),\n",
    "    KeywordExtractor(llm=Settings.llm, keywords=5),\n",
    "]\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "# Các transformations\n",
    "transformations = [splitter] + extractors\n",
    "\n",
    "# Khởi tạo ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=transformations,\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "# Chạy pipeline để xử lý documents\n",
    "nodes = pipeline.run(documents=documents, show_progress=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beutifulize print nodes\n",
    "import json\n",
    "print(\"Nodes:\")\n",
    "for node in nodes:\n",
    "    print(node)\n",
    "\n",
    "    # metadata\n",
    "    # print(\"Metadata:\")\n",
    "    print(json.dumps(node.metadata, indent=2))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_to_qdrant(nodes):\n",
    "    \n",
    "    for node in nodes:\n",
    "        text = node.text\n",
    "\n",
    "        # Get metadata to save as payload dict\n",
    "        metadata = node.metadata\n",
    "        payload = dict(metadata)\n",
    "        \n",
    "        # Create embdding from text\n",
    "        embedding = embed_model._get_text_embedding(text)\n",
    "    \n",
    "\n",
    "        try:\n",
    "            qdrant_client.upsert(\n",
    "                collection_name=\"documents_collection\",\n",
    "                points=[\n",
    "                    PointStruct(\n",
    "                        id=node.id_,\n",
    "                        vector=embedding,\n",
    "                        payload=payload\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            print(\"Dữ liệu đã được lưu vào Qdrant!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to Qdrant: {e}\")\n",
    "# Lưu các nodes vào Qdrant\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=\"documents_collection\",\n",
    "    vectors_config=VectorParams(size=1536, distance=\"Cosine\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_qdrant(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_info = qdrant_client.get_collection(collection_name=collection_name)\n",
    "print(collection_info)\n",
    "\n",
    "vectors = qdrant_client.scroll(\n",
    "    collection_name=\"documents_collection\",\n",
    "    limit=10,\n",
    "    with_payload=True,\n",
    ")\n",
    "\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = []\n",
    "scroll_token = None\n",
    "\n",
    "while True:\n",
    "    # Fetch points in batches\n",
    "    response = qdrant_client.scroll(\n",
    "        collection_name=\"documents_collection\",\n",
    "        with_vectors=True,  # Include vectors in the response\n",
    "        with_payload=True,  # Include payloads in the response\n",
    "        offset=scroll_token,  # Provide the scroll token for pagination\n",
    "    )\n",
    "    \n",
    "    # Add retrieved points to the list\n",
    "    all_points.extend(response[0])\n",
    "    \n",
    "    # Check if there's more data to fetch\n",
    "    scroll_token = response[1]\n",
    "    if scroll_token is None:  # No more data to fetch\n",
    "        break\n",
    "for point in all_points:\n",
    "    print(f\"ID: {point.id}, Vector: {point.vector}, Payload: {point.payload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core import StorageContext\n",
    "# Chuyển đổi danh sách all_points thành danh sách Document\n",
    "documents = [\n",
    "    Document(\n",
    "        id=point.id,  # ID của Document\n",
    "        text=str(point.vector),  # Nội dung vector (hoặc chuyển vector thành chuỗi)\n",
    "        metadata=point.payload  # Thêm metadata\n",
    "    )\n",
    "    for point in all_points\n",
    "]\n",
    "\n",
    "# Thêm vào docstore\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(documents)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=docstore,\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "import Stemmer\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=1,\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "retrieved_nodes = bm25_retriever.retrieve(\n",
    "    \"What do you know?\"\n",
    ")\n",
    "for node in retrieved_nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceEmbeddingOptimizer\n",
    "from llama_index.core.postprocessor import EmbeddingRecencyPostprocessor\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "\n",
    "node_postprocessors = [\n",
    "    SentenceEmbeddingOptimizer(\n",
    "        embed_model=Settings.embed_model,\n",
    "        # percentile_cutoff=0.5,\n",
    "        threshold_cutoff=0.7,\n",
    "    ),\n",
    "    EmbeddingRecencyPostprocessor(date_key=\"date\", similarity_cutoff=0.7),\n",
    "    LLMRerank(top_n=2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "\n",
    "# Configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(llm=Settings.llm, response_mode=ResponseMode.COMPACT)\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=bm25_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=node_postprocessors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "CUSTOM_PROMPT = PromptTemplate(\n",
    "    \"\"\"\n",
    "    Based on the conversation history between the User and the Assistant, along with the User's new question, analyze and understand the question within the context of the conversation.\n",
    "    Provide a relevant response in Vietnamese, using a professional tone like a Human Resource Specialist.  \n",
    "\n",
    "    <Conversation History>\n",
    "    {chat_history}\n",
    "\n",
    "    <Current Question>\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "custom_chat_history = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Hello assistant, we are having a conversation about the company's regulations.\",\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content=\"Great, would you like to know more information about the company's regulations?\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=CUSTOM_PROMPT,\n",
    "    chat_history=custom_chat_history,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Các hình thức xử phạt của công ty?\"\n",
    "# response = query_engine.query(query)\n",
    "response = chat_engine.chat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
